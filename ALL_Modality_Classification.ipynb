{"cells":[{"cell_type":"markdown","id":"a49551f1","metadata":{"id":"a49551f1"},"source":["1. Triplet Input Format : user_id , movie_id , pref_score , rating\n","2. Modality1 : Audio\n","   Modality2 : Video\n","3. Test - Train Split is Defined Within code So, Code Doesnt Requires Seprate File\n","4. Its is for Classification based Problem\n","5. ALL Embeddings Dimension is same\n","6. Here Batch Size for val and Test is compelete set\n","\n","Changes Req\n","\n","1. Path of Input files\n","2. Pah to save checkpoint\n","3. Path of Data File"]},{"cell_type":"markdown","id":"7250dbdd","metadata":{"id":"7250dbdd"},"source":["## Importing Modules\n"]},{"cell_type":"code","execution_count":null,"id":"315dff90","metadata":{"id":"315dff90","outputId":"b0706c9b-6b44-40d1-c9a9-0d8d911dae3b"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pickle5 as pickle\n","import pandas as pd\n","import os\n","import torch.optim as optim\n","import torch\n","from tqdm import tqdm\n","from torchmetrics.functional.classification import multilabel_accuracy\n","import numpy as np\n","from sklearn.metrics import classification_report\n","\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score"]},{"cell_type":"markdown","id":"4029f77a","metadata":{"id":"4029f77a"},"source":["### Support"]},{"cell_type":"code","execution_count":null,"id":"2e657bce","metadata":{"id":"2e657bce"},"outputs":[],"source":["def load_pkl(filename):\n","    filename= filename\n","    data= None\n","    with open(filename, \"rb\") as handle:\n","        data= pickle.load(handle)\n","        handle.close()\n","    return data\n","\n","def store_pkl(object, filename):\n","    filename= path_of(filename)\n","    with open(filename, \"wb\") as handle:\n","        pickle.dump(object, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","        handle.close()\n","\n","\n","def checkpoint(model, filename):\n","    torch.save(model.state_dict(), \"/home/prabir_prj22/Pulkit/MFVCD_7K/Save/\"+filename) #Change\n","\n","def resume(filename):\n","    model.load_state_dict(torch.load(\"/home/prabir_prj22/Pulkit/MFVCD_7K/Save/\"+filename)) #Change"]},{"cell_type":"markdown","id":"2949031a","metadata":{"id":"2949031a"},"source":["### DataSet Loader\n"]},{"cell_type":"code","execution_count":null,"id":"b6c459c4","metadata":{"id":"b6c459c4"},"outputs":[],"source":["from ast import literal_eval\n","class customDataset(torch.utils.data.Dataset):\n","    def __init__(self, data_file ,modality1 , user_modality1 , modality2 , user_modality2):\n","\n","        self.triplet = data_file\n","\n","        self.UserModality1_representation = load_pkl(user_modality1)\n","        self.modality1_representation = load_pkl(modality1)\n","\n","        self.UserModality2_representation = load_pkl(user_modality2)\n","        self.modality2_representation = load_pkl(modality2)\n","\n","\n","    def __len__(self):\n","        return len(self.triplet)\n","\n","    def __getitem__(self, idx):\n","        user_id , movie_id , pref_score , rating  = self.triplet[idx]\n","\n","        #Modality1 Embedding\n","        UserModality1_embed = torch.FloatTensor(self.UserModality1_representation [user_id]).unsqueeze(0)\n","        modality1_embed = pref_score * 100 * torch.FloatTensor(self.modality1_representation[movie_id]).unsqueeze(0)\n","\n","        #Modality2 Embedding\n","        UserModality2_embed = torch.FloatTensor(self.UserModality2_representation [user_id]).unsqueeze(0)\n","        modality2_embed = pref_score * 100 * torch.FloatTensor(self.modality2_representation[movie_id]).unsqueeze(0)\n","\n","\n","\n","        #Label\n","        y_label = torch.FloatTensor(self.onehot(rating[0]))\n","\n","        return movie_id, modality1_embed , UserModality1_embed , modality2_embed , UserModality2_embed , y_label\n","\n","    def onehot(self,rate):\n","        if rate == '-1':\n","            return([1.,0.,0.])\n","\n","        elif rate == '1':\n","            return([0.,0.,1.])\n","\n","        elif rate == '0':\n","            return([0.,1.,0.])\n"]},{"cell_type":"markdown","id":"670ccc55","metadata":{"id":"670ccc55"},"source":["### Report"]},{"cell_type":"code","execution_count":null,"id":"a50311e4","metadata":{"id":"a50311e4"},"outputs":[],"source":["def cal_accuracy(y_test, y_pred):\n","    with open(\"/home/prabir_prj22/Pulkit/User_Modality_Model/Results/xyz.txt\", \"a\") as h: #Change\n","        print(\"\\n\")\n","        # print(\"Confusion Matrix:\\n \",confusion_matrix(y_test, y_pred))\n","        print(\"Confusion Matrix:\\n \",confusion_matrix(y_test, y_pred), file=h)\n","\n","        print (\"Accuracy :\\n \",accuracy_score(y_test,y_pred)*100, file=h)\n","\n","        print(\"Report : \\n\", classification_report(y_test, y_pred), file=h)\n","        # print(\"Report : \\n\", classification_report(y_test, y_pred))\n","        precision = precision_score(y_test, y_pred, average = 'micro')\n","        print('Precision: %f' % precision, file=h)\n","        # recall: tp / (tp + fn)\n","        recall = recall_score(y_test, y_pred, average = 'micro')\n","        print('Recall: %f' % recall, file=h)\n","        # f1: 2 tp / (2 tp + fp + fn)\n","        f1 = f1_score(y_test, y_pred, average = 'micro')\n","        print('F1 score: %f' % f1, file=h)"]},{"cell_type":"markdown","id":"b7507323","metadata":{"id":"b7507323"},"source":["### Loading Files"]},{"cell_type":"code","execution_count":null,"id":"d56fbb0d","metadata":{"id":"d56fbb0d","outputId":"6a66229e-103c-4745-f7c7-5484b9632262","scrolled":true},"outputs":[],"source":["#Change\n","\n","#Movie Embeddings\n","\n","modality1 = \"/home/prabir_prj22/Pulkit/User_Modality_Model/Dataset/w2v/wave2vec_movie_audio.pkl\"\n","modality2 = \"/home/prabir_prj22/Pulkit/User_Modality_Model/Dataset/scenedetection_object_detection_video_embeddings/scenedetection_object_detection_video_embeddings.pkl\"\n","\n","#User Embeddings\n","user_modality1 = \"/home/prabir_prj22/Pulkit/User_Modality_Model/Dataset/w2v/user_emb_all_Score.pkl\"\n","user_modality2 = \"/home/prabir_prj22/Pulkit/User_Modality_Model/Dataset/scenedetection_object_detection_video_embeddings/user_emb_all_Score.pkl\"\n","\n","#Data Triplet\n","data_file_final = load_pkl(\"/home/prabir_prj22/Pulkit/User_Modality_Model/Dataset/Others/data_file_final.pkl\") #Change\n","\n","print(len(data_file_final))"]},{"cell_type":"markdown","id":"3cfeefd8","metadata":{"id":"3cfeefd8"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"id":"22788899","metadata":{"id":"22788899"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class CrossAttention(nn.Module):\n","    def __init__(self, input_dim:int ,\n","                 num_heads:int ,\n","                 dropout:int\n","                ):\n","        super(CrossAttention, self).__init__()\n","\n","        # Multi-head attention layer\n","        self.attention = nn.MultiheadAttention(input_dim, num_heads)\n","        self.norm = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input1, input2):\n","        # Compute the context vectors for both inputs\n","        context1, _ = self.attention(input1, input2, input2)\n","\n","        # Compute the outputs for both inputs\n","        output1 = self.norm (input1 + self.dropout(context1))\n","\n","        return output1\n","\n","# Define the classification model\n","class ClassificationModel(nn.Module):\n","    def __init__(self,  modality1_len , modality2_len , output_dim=3, dropout=0.0):\n","        super(ClassificationModel, self).__init__()\n","\n","#         print(modality1_len)\n","#         print(modality2_len)\n","\n","        #modality1\n","        self.cross_attention_modality1_user = CrossAttention(input_dim = modality1_len, num_heads=4, dropout=0.0)\n","        self.cross_attention_user_modality1 = CrossAttention(input_dim = modality1_len, num_heads=4, dropout=0.0)\n","\n","        #modality2\n","        self.cross_attention_modality2_user = CrossAttention(input_dim = modality2_len, num_heads=4, dropout=0.0)\n","        self.cross_attention_user_modality2 = CrossAttention(input_dim = modality2_len, num_heads=4, dropout=0.0)\n","\n","        #All\n","        self.cross_attention_concat = CrossAttention(input_dim = (modality1_len + modality2_len)*2, num_heads=4, dropout=0.0)\n","\n","        #Feed Forward layers\n","        self.feedForward = nn.Sequential(\n","                nn.Linear((modality1_len+modality2_len)*2, (modality1_len+modality2_len)),\n","                nn.ReLU(),\n","                nn.Linear((modality1_len+modality2_len), (modality1_len+modality2_len)*2)\n","            )\n","\n","        # Regression layers\n","        self.classify1 = nn.Linear((modality1_len+modality2_len)*2, (modality1_len+modality2_len))\n","        self.classify2 = nn.Linear((modality1_len+modality2_len), (modality1_len+modality2_len)//2)\n","        self.classify3 = nn.Linear((modality1_len+modality2_len)//2, output_dim)\n","\n","        # norm layer\n","        self.norm = nn.LayerNorm((modality1_len+modality2_len)*2)\n","\n","    def forward(self,  modality1_embed , UserModality1_embed , modality2_embed , UserModality2_embed):\n","\n","        # Compute the outputs of the cross attention model\n","\n","        #modality1\n","        output1 = self.cross_attention_modality1_user (modality1_embed, UserModality1_embed)\n","        output2 = self.cross_attention_user_modality1(UserModality1_embed, modality1_embed)\n","\n","        output_concat1 = torch.cat((output1,output2), 2)\n","\n","\n","        #modality2\n","        output3 = self.cross_attention_modality2_user(modality2_embed, UserModality2_embed)\n","        output4 = self.cross_attention_user_modality2(UserModality2_embed, modality2_embed)\n","        output_concat2 = torch.cat((output3,output4), 2)\n","\n","        #All-attention\n","        output_concat = torch.cat((output_concat1,output_concat2), 2)\n","        final_output = self.cross_attention_concat(output_concat, output_concat)\n","\n","        feed_out = self.feedForward(final_output)\n","        output = self.norm(final_output+feed_out)\n","\n","\n","        output = self.classify1(output)\n","        output = nn.functional.relu(output)\n","        output = self.classify2(output)\n","        output = nn.functional.relu(output)\n","        output = self.classify3(output).squeeze(1)\n","\n","        return output\n","\n","        #Here Output is of Classification type"]},{"cell_type":"markdown","id":"24179efb","metadata":{"id":"24179efb"},"source":["### Train Function"]},{"cell_type":"code","execution_count":null,"id":"f9731928","metadata":{"id":"f9731928"},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, train_data, device):\n","    print('Training')\n","    model.train()\n","    counter = 0\n","    train_running_loss = 0.0\n","    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n","        counter += 1\n","        movie_id, x_modality1, x_UserModality1, x_modality2, x_UserModality2, target = data\n","\n","        x_modality1 =  x_modality1.to(device)\n","        x_UserModality1 = x_UserModality1.to(device)\n","        x_modality2 =  x_modality2.to(device)\n","        x_UserModality2 = x_UserModality2.to(device)\n","        target = target.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(x_modality1, x_UserModality1, x_modality2, x_UserModality2)\n","#         print(outputs.shape)\n","#         print(target.shape)\n","        loss = criterion(outputs, target)\n","        train_running_loss += loss.item()\n","\n","        # backpropagation\n","        loss.backward()\n","\n","        # update optimizer parameters\n","        optimizer.step()\n","\n","    train_loss = train_running_loss / counter\n","    return train_loss"]},{"cell_type":"markdown","id":"c4b144f9","metadata":{"id":"c4b144f9"},"source":["### Test-Function"]},{"cell_type":"code","execution_count":null,"id":"81a4ba8a","metadata":{"id":"81a4ba8a"},"outputs":[],"source":["def validate(model, dataloader, criterion, val_data, device):\n","    print('Validating')\n","    model.eval()\n","    counter = 0\n","    val_running_loss = 0.0\n","    running_accuracy = 0.0\n","\n","    with torch.no_grad():\n","        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n","            counter += 1\n","            movie_id, x_modality1, x_UserModality1, x_modality2, x_UserModality2, target = data\n","\n","            x_modality1 =  x_modality1.to(device)\n","            x_UserModality1 = x_UserModality1.to(device)\n","            x_modality2 =  x_modality2.to(device)\n","            x_UserModality2 = x_UserModality2.to(device)\n","            target = target.to(device)\n","\n","            outputs = model(x_modality1, x_UserModality1, x_modality2, x_UserModality2)\n","            loss = criterion(outputs, target)\n","            val_running_loss += loss.item()\n","\n","            #Calculating Accuracy\n","            accuracy = (torch.argmax(outputs, 1) == torch.argmax(target, 1)).float().mean()\n","\n","        val_loss = val_running_loss / counter\n","        return val_loss , accuracy , target , outputs"]},{"cell_type":"code","execution_count":null,"id":"7e2146f9","metadata":{"id":"7e2146f9","outputId":"49e973c6-792b-4778-e887-92d950213058"},"outputs":[],"source":["device = torch.device(\"cuda:0\")\n","print(device)\n"]},{"cell_type":"code","execution_count":null,"id":"7f856edc","metadata":{"id":"7f856edc","outputId":"61a829f6-6e39-47f1-b4c9-8eb2aea98e80","scrolled":true},"outputs":[],"source":["#Name Of file to save model\n","save_path = \"best_model_ALL.pth\"\n","\n","# learning parameters\n","from torch.optim.lr_scheduler import StepLR\n","early_stop_thresh = 15\n","epochs = 150\n","\n","print(\"Model Transfered\")\n","\n","\n","#Train - Test Split\n","X = []\n","Y = []\n","for sample in data_file_final:\n","    X.append(sample)\n","    Y.append(int(sample[3][0]))\n","\n","import collections\n","# using Counter to find frequency of elements\n","frequency = collections.Counter(Y)\n","print(dict(frequency))\n","\n","\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","\n","for i, (train_index, test_index) in enumerate(sss.split(X, Y)):\n","    train_data_X = []\n","    test_data = []\n","\n","    for trn_indx in train_index:\n","        train_data_X.append(X[trn_indx])\n","\n","    for tst_indx in test_index:\n","        test_data.append(X[tst_indx])\n","\n","\n","#Train - Val Split\n","X = []\n","Y = []\n","for sample in train_data_X:\n","    X.append(sample)\n","    Y.append(int(sample[3][0]))\n","\n","sss1 = StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=0)\n","\n","\n","#Running for 5 Splits\n","for i, (train_index, val_index) in enumerate(sss1.split(X, Y)):\n","    best_loss = 1000\n","    best_epoch = -1\n","    valid_epoch_loss = -1\n","    lr = 0.00001\n","\n","\n","    train_data = []\n","    val_data = []\n","    valid_acc = []\n","\n","    for trn_indx in train_index:\n","        train_data.append(X[trn_indx])\n","\n","    for val_indx in val_index:\n","        val_data.append(X[val_indx])\n","\n","    #Definig Loaders\n","    train_set = customDataset(train_data, modality1, user_modality1, modality2, user_modality2)\n","    val_set = customDataset(val_data, modality1, user_modality1, modality2, user_modality2)\n","    test_set = customDataset(test_data, modality1, user_modality1, modality2, user_modality2)\n","\n","    print(\"Data in Train set:\" , len(train_set))\n","    print(\"Data in Val set:\" , len(val_set))\n","    print(\"Data in Test set:\" , len(test_set))\n","\n","    train_loader = torch.utils.data.DataLoader(dataset = train_set , batch_size=32, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(dataset = val_set , batch_size=len(val_set))\n","\n","    #Calculating the Dimesion of embeddings\n","    modality1_len = len(next(iter(train_set))[1][0])\n","    modality2_len = len(next(iter(train_set))[3][0])\n","\n","    #Trainig Loop\n","    model = ClassificationModel(modality1_len , modality2_len)\n","    model = model.to(device)\n","    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n","    scheduler = StepLR(optimizer,\n","                   step_size = 50, # Period of learning rate decay\n","                   gamma = 0.1) # Multiplicative factor of learning rate decay\n","    criterion = nn.CrossEntropyLoss()  # mean square error\n","\n","    #Contains train_loss and valid_loss for the complete epochs\n","    train_loss = []\n","    valid_loss = []\n","    valid_acc = []\n","\n","    for epoch in range(epochs):\n","        scheduler.step()\n","        print(f\"Epoch {epoch+1} of {epochs}\" , \"Learnig Rate: \" ,scheduler.get_last_lr())\n","\n","        #CALLING TRAINING FUNCTION\n","        train_epoch_loss = train(\n","            model, train_loader, optimizer, criterion, train_set, device\n","        )\n","\n","         #CALLING VALIDATION FUNCTION\n","        valid_epoch_loss , accuracy_score_epoch , target , outputs = validate(\n","            model, val_loader, criterion, val_set, device\n","        )\n","\n","        train_loss.append(train_epoch_loss)\n","        valid_loss.append(valid_epoch_loss)\n","        valid_acc.append(accuracy_score_epoch.item())\n","\n","        print(f\"Train Loss: {train_epoch_loss:.4f}\")\n","        print(f'Val Loss: {valid_epoch_loss:.4f}')\n","        print(f'Val Acc: {accuracy_score_epoch:.4f}')\n","\n","        if valid_epoch_loss < best_loss:\n","            best_loss = valid_epoch_loss\n","            best_epoch = epoch\n","            checkpoint(model, save_path)\n","\n","        elif epoch - best_epoch > early_stop_thresh:\n","            print(\"Early stopped training at epoch %d\" % epoch)\n","            break  # terminate the training loop\n","\n","    #Testing The model\n","    resume(save_path)\n","    model.to(device)\n","    test_loader = torch.utils.data.DataLoader(dataset = test_set , batch_size=len(test_set))\n","    print(len(test_set))\n","\n","    test_epoch_loss , Testaccuracy_score_epoch, target , outputs = validate(\n","            model, test_loader, criterion, test_set, device\n","        )\n","\n","    y_true = target.cpu().data.numpy().argmax(axis=-1)\n","    y_pred = outputs.cpu().data.numpy().argmax(axis=-1)\n","\n","    cal_accuracy(y_true, y_pred)\n","    print(f'Test Loss: {test_epoch_loss:.4f}')\n","    print(f'Test Acc: {Testaccuracy_score_epoch:.4f}')\n"]},{"cell_type":"code","execution_count":null,"id":"JMcwwlupu9By","metadata":{"id":"JMcwwlupu9By"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"prabir","language":"python","name":"prabir"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
