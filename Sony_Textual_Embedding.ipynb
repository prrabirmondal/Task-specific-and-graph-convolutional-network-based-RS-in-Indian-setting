{"cells":[{"cell_type":"markdown","metadata":{"id":"e9h-XtndpVbO"},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvdKCH2IJLoq","outputId":"e3aa8816-c37c-42a5-dff9-57faa6bf2c59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting googletrans==3.1.0a0\n","  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3\n","  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpcore==0.9.*\n","  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hstspreload\n","  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.12.7)\n","Collecting sniffio\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n","Collecting rfc3986<2,>=1.3\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Collecting chardet==3.*\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h2==3.*\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.10,>=0.8\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperframe<6,>=5.2.0\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting hpack<4,>=3.0\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16368 sha256=6b9701b9aafe30a0e161366b51fb3e4e1f37d8fe5dbcba299313498db7427013\n","  Stored in directory: /root/.cache/pip/wheels/dd/59/af/8d6c96a719763990f1c548e36b17d9efdfb767f42f7ff39f53\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, sniffio, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 4.0.0\n","    Uninstalling chardet-4.0.0:\n","      Successfully uninstalled chardet-4.0.0\n","Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.4.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=d0fe37b4cd760114693d1665f66fa5650c452a8203bd5a4490ab06a555d74fc7\n","  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n","Successfully built sentence-transformers\n","Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n","Successfully installed huggingface-hub-0.12.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip3 install googletrans==3.1.0a0\n","!pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4hBidhRo3VH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ccb9b4fe-32e1-4d6c-a89d-730ac867c1ba"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}],"source":["import json\n","import pandas as pd\n","import os\n","import gzip\n","import pickle\n","from pathlib import Path\n","from os import listdir\n","from os.path import isfile, join\n","import spacy\n","from spacy.matcher import Matcher\n","from tqdm import tqdm\n","import googletrans\n","from pprint import pprint\n","\n","\n","import pandas as pd\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from sentence_transformers import SentenceTransformer"]},{"cell_type":"markdown","metadata":{"id":"zptdBszro-Qq"},"source":["# Helper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nemaw_LS7Yd"},"outputs":[],"source":["nlp = spacy.load('en_core_web_sm')\n","\n","\n","def path_of(location):\n","    me_dir, me_file= os.path.split(os.path.abspath(__file__))\n","    return os.path.join(me_dir, location)\n","\n","\n","def get_all_files(location, get_full_path= False):\n","    location= path_of(location)\n","    onlyfiles = [f for f in listdir(location) if isfile(join(location, f))]\n","    if get_full_path:\n","        onlyfiles= [join(location, f) for f in onlyfiles]\n","    return onlyfiles\n","\n","\n","def load_pkl(filename):\n","    filename= filename\n","    data= None\n","    with open(filename, \"rb\") as handle:\n","        data= pickle.load(handle)\n","        handle.close()\n","    return data\n","\n","def store_pkl(object, filename):\n","    filename= filename\n","    with open(filename, \"wb\") as handle:\n","        pickle.dump(object, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","        handle.close()\n","\n","def is_valid_file(filename):\n","    filename= path_of(filename)\n","    file= Path(filename)\n","    if file.is_file():\n","        return True\n","    return False\n","\n","\n","def to_same_shape(arr_of_items, required_shape):\n","    if len(arr_of_items) == 0:\n","        print(\"Error: tried to make shape: \", required_shape, \", but item is empty...\")\n","        exit()\n","    if len(arr_of_items)>required_shape:\n","        return arr_of_items[:required_shape]\n","    res= []\n","    ind=0\n","    while len(res)<required_shape:\n","        res.append(arr_of_items[ind])\n","        ind= (ind+1)%len(arr_of_items)\n","    return res"]},{"cell_type":"markdown","metadata":{"id":"JPtkR6CfpBts"},"source":["# Meta Information encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kyV45ZBioCYj"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","#import helpers\n","\n","vector_file= \"/content/drive/MyDrive/sony_IITPatna/word_vector_300d.vec\"\n","hindi = \"/content/drive/MyDrive/Frames/cc.hi.300.vec\"\n","\n","def get_unique_words(ctnr:set, phases:list):\n","    for words in phases:\n","        for word in words:\n","            if len(word) > 0:\n","                ctnr.add(word)\n","\n","def get_vector(word_vec:dict, words:set):\n","    with open(vector_file) as file:\n","        for line in tqdm(file):\n","            word_and_vals= line.split(\" \")\n","            w= word_and_vals[0]\n","            if w in words:\n","                vec= word_and_vals[1:]\n","                vec= list(map(float, vec))\n","                vec= np.array(vec, dtype=float)\n","                word_vec.update({\n","                    w: vec\n","                })\n","    file.close()\n","\n","def split_by_words(arr:list):\n","    res= []\n","    for ele in arr:\n","        words= ele.split(\" \")\n","        r= list()\n","        for w in words:\n","            if len(w)>0:\n","                r.append(w)\n","        res.append(r)\n","    return res\n","\n","def avg_vec(words_arr:list, word_vec:dict):\n","    res= np.zeros(300)\n","    found_words= 0\n","    for w in words_arr:\n","        if not word_vec.get(w) is None:\n","            res= res+word_vec.get(w)\n","            found_words+=1\n","    if found_words>0:\n","        res= res/found_words\n","    return res\n","\n","\n","def transE_single(head, relation, tail, word_vec):\n","    head_v= avg_vec(head, word_vec)\n","    relation_v= avg_vec(relation, word_vec)\n","    tail_v= avg_vec(tail, word_vec)\n","    enc= head_v+relation_v-tail_v\n","    return enc\n","\n","\n","def transE(head:list, relation:list, tail:list, word_vec:dict):\n","    if len(head) != len(relation) or len(head)!= len(tail):\n","        raise \"\\n\\tError!! head, tail, relation doesnot have same length\\n\"\n","    res= []\n","    for index in range(len(head)):\n","        vec= transE_single(head[index],relation[index], tail[index], word_vec)\n","        res.append(vec)\n","    res= np.array(res)\n","    return res\n","\n","def encode(head:list, relation:list, tail:list):\n","    unique_words= set()\n","\n","    head= split_by_words(head)\n","    relation= split_by_words(relation)\n","    tail= split_by_words(tail)\n","\n","    get_unique_words(unique_words, head)\n","    get_unique_words(unique_words, relation)\n","    get_unique_words(unique_words, tail)\n","\n","    word_vec= dict()\n","    get_vector(word_vec, unique_words)\n","\n","    res= transE(head, relation, tail, word_vec)\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GswwlMXroUIy"},"outputs":[],"source":["def get_entities(sent):\n","    ent1 = \"\"\n","    ent2 = \"\"\n","\n","    prv_tok_dep = \"\"\n","    prv_tok_text = \"\"\n","\n","    prefix = \"\"\n","    modifier = \"\"\n","\n","\n","    for tok in nlp(sent):\n","        if tok.dep_ != \"punct\":\n","            if tok.dep_ == \"compound\":\n","                prefix = tok.text\n","                if prv_tok_dep == \"compound\":\n","                    prefix = prv_tok_text + \" \"+ tok.text\n","\n","            if tok.dep_.endswith(\"mod\") == True:\n","                modifier = tok.text\n","                if prv_tok_dep == \"compound\":\n","                    modifier = prv_tok_text + \" \"+ tok.text\n","\n","            if tok.dep_.find(\"subj\") == True:\n","                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n","                prefix = \"\"\n","                modifier = \"\"\n","                prv_tok_dep = \"\"\n","                prv_tok_text = \"\"\n","\n","            if tok.dep_.find(\"obj\") == True:\n","                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n","\n","            prv_tok_dep = tok.dep_\n","            prv_tok_text = tok.text\n","\n","    return [ent1.strip(), ent2.strip()]\n","\n","\n","def get_relation(sent):\n","    doc = nlp(sent)\n","\n","    matcher = Matcher(nlp.vocab)\n","\n","    pattern =           [{'DEP':'ROOT'},\n","                        {'DEP':'prep','OP':\"?\"},\n","                        {'DEP':'agent','OP':\"?\"},\n","                        {'POS':'ADJ','OP':\"?\"}]\n","\n","    matcher.add(\"matching_1\", None, pattern)\n","    matches = matcher(doc)\n","    k = len(matches) - 1\n","    span = doc[matches[k][1]:matches[k][2]]\n","    return(span.text)\n","\n","\n","def get_elements(sent_arr):\n","    head= []\n","    tail= []\n","    relation= []\n","\n","    for sen in tqdm(sent_arr):\n","        try:\n","            entity= get_entities(sen)\n","            rel= get_relation(sen)\n","        except:\n","            entity= [\"None\", \"None\"]\n","            rel= \"None\"\n","        head.append(entity[0])\n","        tail.append(entity[1])\n","        relation.append(rel)\n","    return head, relation, tail\n","\n","\n","def draw_graph(head, relation, tail):\n","    kg_df = pd.DataFrame({'source':head, 'target':tail, 'edge':relation})\n","    G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph())\n","    plt.figure(figsize=(12,12))\n","    pos = nx.spring_layout(G)\n","    nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n","    plt.show()\n","\n","\n","def inner_preprocess_sen(sen:str):\n","    sen= sen.rstrip(\"\\n\").lstrip(\" \").rstrip(\" \").lstrip(\"\\\"\").rstrip(\"\\\"\")\n","    return sen\n","\n","def preprocess_sen(sen):\n","    if type(sen) == str:\n","        return inner_preprocess_sen(sen)\n","    elif type(sen) == list:\n","        res= list()\n","        for s in sen:\n","            res.append(inner_preprocess_sen(s))\n","        return res\n","    else:\n","        raise \"\\n\\tPreprocess sen takes string or list of string...\\n\"\n","\n","def build_and_encode(sentence_arr:list):\n","    sen_arr= []\n","    for sen in sentence_arr:\n","        sen= preprocess_sen(sen)\n","        sen_arr.append(sen)\n","    h,r,t= get_elements(sen_arr)\n","    #import kg_encoder as kg_encoder\n","    encoded= encode(h,r,t)\n","    return encoded"]},{"cell_type":"markdown","metadata":{"id":"_8o9kHMdnqWf"},"source":["# Extract Meta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jwfkj9SaS6Hd"},"outputs":[],"source":["with open('/content/drive/MyDrive/fickscore/movies.json') as f:\n","    data = [json.loads(line) for line in f]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9vNpLaoU48r"},"outputs":[],"source":["director = []\n","rating = []\n","cast = []\n","summary = []\n","movie_id = []\n","name = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKb4c1thXo5u"},"outputs":[],"source":["genere = []\n","c = 0\n","for i in data:\n","  c = c + 1\n","  genere.append(i['genre']) # Making list of genres of all movies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMyfvF6PcqVQ"},"outputs":[],"source":["final_genere = []        # Doing one hot encoding of genres of all movies\n","d = 0\n","for j in genere:\n","\n","  l = [0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","  for i in j:\n","    if i == 'Biography':\n","         l[0] = 1\n","    if i == 'Musical':\n","         l[1] = 1\n","    if i == 'Mystery':\n","         l[2] = 1\n","    if i == 'Adventure':\n","         l[3] = 1\n","    if i == 'Action':\n","         l[4] = 1\n","    if i == 'War':\n","         l[5] = 1\n","    if i == 'Music':\n","         l[6] = 1\n","    if i == 'Fantasy':\n","         l[7] = 1\n","    if i == 'Romance':\n","         l[8] = 1\n","    if i == 'Thriller':\n","         l[9] = 1\n","    if i == 'Family':\n","         l[10] = 1\n","    if i == 'History':\n","         l[11] = 1\n","    if i == 'Crime':\n","         l[12] = 1\n","    if i == 'Drama':\n","         l[13] = 1\n","  d = d + 1\n","  final_genere.append(l)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmSvwWPsTaqo"},"outputs":[],"source":["for i in data:\n","  #if i['movie_id'] in dict1:\n","      director.append(i['director'])\n","      rating.append(i['rating'])\n","      cast.append(i['cast'])\n","      summary.append(i['description'])\n","      movie_id.append(i['movie_id'])\n","      name.append(i['name'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2h9Ng54kkWl9","outputId":"9d708921-e6f1-4e0e-b7ab-338db870ceae"},"outputs":[{"output_type":"stream","name":"stdout","text":["skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n","skiping_for_casts\n"]}],"source":["\"\"\"\n","Creating a dictionary to store meta information of movies in the following format:\n","{\n","  \"name\": movie name,\n","  \"genre\": movie genres,\n","  \"director\": movie director,\n","  \"rating\": movie rating,\n","  \"casts\": movie casts,\n","  \"summary\": movie summary,\n","}\n","\n","\"\"\"\n","op_dict = dict()\n","count = 0\n","for i in range(2850):\n","  #print(director[i])\n","  if len(director[i])!= 0 :\n","     dir = director[i][0]\n","  else:\n","    dir = \"None\"\n","  if len(dir)==0:\n","            print(\"skiping_for_director\")\n","            continue\n","  rat = rating[i]\n","  if len(rating)==0:\n","            print(\"skiping_for_ratings\")\n","            continue\n","  genres = final_genere[i]\n","  rat = rating[i]\n","  nam = name[i]\n","  casts = cast[i]\n","  if len(casts) == 0:\n","            print(\"skiping_for_casts\")\n","            continue\n","  # casts = cast[i]\n","  summ = summary[i]\n","  id = movie_id[i]\n","  if len(summary[i])!=0 and len(genres)!=0  and len(cast[i])!=0:\n","    count = count + 1\n","    op_dict.update({\n","            id: {\n","                \"name\": nam,\n","                \"genre\": genres,\n","                \"director\": dir,\n","                \"rating\": rat,\n","                \"casts\": casts,\n","                \"summary\": summ,\n","            }\n","        })"]},{"cell_type":"markdown","metadata":{"id":"KNJDrsIInvmC"},"source":["# Process Meta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYp5RTdvngBb"},"outputs":[],"source":["meta_raw= op_dict\n","\n","translator = googletrans.Translator()\n","\n","meta_processed_op_oath= \"latest_sentence_fickscore_meta_processed_v2.obj\" # Path where you want to save your textual embedding\n","\n","w2v_file_path = \"/content/drive/MyDrive/sony_IITPatna/word_vector_300d.vec\"\n","\n","hindi = \"/content/drive/MyDrive/Frames/cc.hi.300.vec\"\n","\n","model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n","\n","lines_to_take= 3\n","\n","\n","def filter_and_preprocess_lines(lines_arr):\n","    res= list()\n","    for l in lines_arr:\n","        l= l.lstrip(\" \").rstrip(\" \")\n","        if len(l)==0: continue\n","        if len(l.split(\" \"))<4: continue\n","        res.append(l)\n","    return res\n","\n","def concat_encoded(encoded_arr):\n","    res= list()\n","    for item in encoded_arr:\n","        res.extend(item)\n","    return res\n","\n","def avg_encode(encoded_arr):\n","    res= [0 for x in range(len(encoded_arr[0]))]\n","    for enc in encoded_arr:\n","        for ind, val in enumerate(enc):\n","            res[ind]+=val\n","    for index in range(len(res)):\n","        res[index]= res[index]/len(encoded_arr)\n","    return res\n","\n","def encode_description(meta_raw):\n","\n","    \"\"\"\n","    Function to encode movie description\n","    Input: Meta information\n","    Output: Encoded vector of length 384\n","    \"\"\"\n","\n","    all_lines= []\n","    all_movie_id= list(meta_raw.keys())\n","    res = dict()\n","    # print(all_movie_id)\n","\n","    lines_to_take_while_encoding= 8\n","\n","    for movie_id in all_movie_id:\n","        movie_meta= meta_raw.get(movie_id)\n","        description= movie_meta.get(\"summary\")\n","        lines_raw= description.split(\".\")\n","        lines_filtered= filter_and_preprocess_lines(lines_raw)\n","        encoded_lines = model.encode(lines_filtered)\n","        final_encoded = np.average(encoded_lines,axis=0)\n","        res.update({\n","            movie_id: final_encoded,\n","        })\n","\n","    return res\n","\n","\n","def encode_director(meta_raw):\n","\n","    \"\"\"\n","    Function to encode director name\n","    Input: Meta information\n","    Output: Encoded vector of length 300\n","    \"\"\"\n","\n","    all_movie_id= list(meta_raw.keys())\n","\n","    director_split_vectors= dict()\n","\n","    for movie_id in all_movie_id:\n","        movie_meta= meta_raw.get(movie_id)\n","        director_name= movie_meta.get(\"director\")\n","        results = translator.translate(director_name,dest='hi')\n","        director_split= results.text.split(\" \")\n","        for d in director_split:\n","            d= d.lower()\n","            director_split_vectors.update({\n","                d: False\n","            })\n","\n","    with open(hindi) as file:\n","        for line in file:\n","            line= line.rstrip(\"\\n\")\n","            items= line.split(\" \")\n","            word= items[0].lower()\n","            if director_split_vectors.get(word) is not None:\n","                vector= [float(x) for x in items[1:]]\n","                director_split_vectors.update({\n","                    word: vector,\n","                })\n","        file.close()\n","\n","    director_encoding = dict()\n","\n","    for movie_id in all_movie_id:\n","        movie_meta= meta_raw.get(movie_id)\n","        director_name= movie_meta.get(\"director\")\n","        results = translator.translate(director_name,dest='hi')\n","        director_split= results.text.split(\" \")\n","        enc= list()\n","        for d in director_split:\n","            d= d.lower()\n","            if director_split_vectors.get(d):\n","                enc.append(director_split_vectors.get(d))\n","            else:\n","                enc.append(list(np.random.uniform(-0.2,0.2,300)))\n","\n","        enc= to_same_shape(enc, 2)\n","        enc= avg_encode(enc)\n","\n","        director_encoding.update({\n","            movie_id: enc,\n","        })\n","\n","    return director_encoding\n","\n","def encode_movie_name(meta_raw):\n","\n","    \"\"\"\n","    Function to encode movie name\n","    Input: Meta information\n","    Output: Encoded vector of length 300\n","    \"\"\"\n","\n","    all_movie_id= list(meta_raw.keys())\n","\n","    movie_split_vectors= dict()\n","\n","    for movie_id in all_movie_id:\n","        movie_meta= meta_raw.get(movie_id)\n","        movie_name= movie_meta.get(\"name\")\n","        results = translator.translate(movie_name,dest='hi')\n","        movie_split= results.text.split(\" \")\n","        for d in movie_split:\n","            d= d.lower()\n","            movie_split_vectors.update({\n","                d: False\n","            })\n","\n","    with open(hindi) as file:\n","        for line in file:\n","            line= line.rstrip(\"\\n\")\n","            items= line.split(\" \")\n","            word= items[0].lower()\n","            if movie_split_vectors.get(word) is not None:\n","                vector= [float(x) for x in items[1:]]\n","                movie_split_vectors.update({\n","                    word: vector,\n","                })\n","        file.close()\n","\n","    movie_encoding = dict()\n","\n","    for movie_id in all_movie_id:\n","        movie_meta= meta_raw.get(movie_id)\n","        movie_name= movie_meta.get(\"name\")\n","        results = translator.translate(movie_name,dest='hi')\n","        movie_split= results.text.split(\" \")\n","        enc= list()\n","        for d in movie_split:\n","            d= d.lower()\n","            if movie_split_vectors.get(d):\n","                enc.append(movie_split_vectors.get(d))\n","            else:\n","                enc.append(list(np.random.uniform(-0.2,0.2,300)))\n","\n","        enc= to_same_shape(enc, 2)\n","        enc= avg_encode(enc)\n","        movie_encoding.update({\n","            movie_id: enc,\n","        })\n","    return movie_encoding\n","\n","def encode_genre(meta_raw):\n","\n","    \"\"\"\n","    Function to encode movie genre\n","    Input: Meta information\n","    Output: One hot encoded vector of length 15\n","    \"\"\"\n","\n","\n","    all_movie_id= list(meta_raw.keys())\n","    genre_encoded= dict()\n","    for movie_id in all_movie_id:\n","        movie_meta= meta_raw.get(movie_id)\n","        genre= movie_meta.get(\"genre\")\n","        genre_encoded.update({\n","            movie_id: genre,\n","        })\n","    return genre_encoded\n","\n","\n","def encode_rating(meta_raw):\n","\n","    \"\"\"\n","    Function to normalize movie rating\n","    Input: Meta information\n","    Output: 1 single float value\n","    \"\"\"\n","    all_movie_id= list(meta_raw.keys())\n","    rating_encoded= dict()\n","    for movie_id in all_movie_id:\n","        movie_meta= meta_raw.get(movie_id)\n","        rating= movie_meta.get(\"rating\")\n","        rating= rating/10\n","        rating_encoded.update({\n","            movie_id: [rating],\n","        })\n","    return rating_encoded\n","\n","\n","def encode_everything():\n","\n","    \"\"\"\n","    Function to encode description, movie name, director, genre, and rating\n","    Input: Meta information\n","    Output: Encoded vector of dimension 999 (14 + 1 + 300 + 300 + 384)\n","    \"\"\"\n","\n","    enc_desc= encode_description(meta_raw)\n","    print(\"[*] Description encoding completed...\\n\")\n","    enc_name= encode_movie_name(meta_raw)\n","    print(\"[*] Name encoding completed...\\n\")\n","    enc_direc= encode_director(meta_raw)\n","    print(\"[*] Director encoding completed...\\n\")\n","    enc_genre= encode_genre(meta_raw)\n","    print(\"[*] Genre encoding completed...\\n\")\n","    enc_rate= encode_rating(meta_raw)\n","    print(\"[*] Rating encoding completed...\\n\")\n","\n","    metadata_encoded= dict()\n","\n","    all_movie_id= list(meta_raw.keys())\n","    for movie_id in all_movie_id:\n","        de= enc_desc.get(movie_id)\n","        ge= enc_genre.get(movie_id)\n","        di= enc_direc.get(movie_id)\n","        ra= enc_rate.get(movie_id)\n","        na = enc_name.get(movie_id)\n","\n","        enc_concated= list()\n","        enc_concated.extend(de)\n","        enc_concated.extend(ge)\n","        enc_concated.extend(di)\n","        enc_concated.extend(ra)\n","        enc_concated.extend(na)\n","\n","        metadata_encoded.update({\n","            movie_id: enc_concated,\n","        })\n","\n","    store_pkl(metadata_encoded, meta_processed_op_oath)\n","    return metadata_encoded,enc_desc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMgovjLRf0bj","outputId":"cd225f98-e246-44c1-e584-d8f51b20e31b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[*] Description encoding completed...\n","\n","[*] Name encoding completed...\n","\n","[*] Director encoding completed...\n","\n","[*] Genre encoding completed...\n","\n","[*] Rating encoding completed...\n","\n"]}],"source":["a = encode_everything()"]},{"cell_type":"code","source":["import pickle\n","file_pi = open('/content/drive/MyDrive/sony_IITPatna/latest_text_emvedding.obj', 'wb')\n","pickle.dump(a, file_pi)"],"metadata":{"id":"ir9zuMnvsNk4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bnm = load_pkl(\"/content/drive/MyDrive/sony_IITPatna/latest_text_emvedding.obj\")"],"metadata":{"id":"AChAxxlpwJPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(bnm['tt1520485'])"],"metadata":{"id":"5KLW2PWhwElQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["e9h-XtndpVbO","zptdBszro-Qq","JPtkR6CfpBts","_8o9kHMdnqWf","KNJDrsIInvmC"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}